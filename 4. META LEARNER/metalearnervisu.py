# -*- coding: utf-8 -*-
"""metalearnervisu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AXh4KlWm6_8EkyqmeL_tJ7vyFkLNv04y
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import joblib

# Load model predictions for meta-learning
data = pd.read_csv('combined_predictions.csv')  # This CSV contains columns for individual model predictions

X = data[['cnn_pred', 'lstm_pred', 'bert_pred', 'cnn_lstm_pred', 'bert_bilstm_pred']]
y = data['true_label']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the logistic regression meta-learner
meta_learner = LogisticRegression(max_iter=1000)
meta_learner.fit(X_train, y_train)

from sklearn.metrics import classification_report

# Predict on the test set to get the confusion matrix
y_test_pred = meta_learner.predict(X_test)
# Get precision, recall, and F1-score
print(classification_report(y_test, y_test_pred, digits=4))

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import classification_report

# Get the classification report as a dictionary
report = classification_report(y_test, y_test_pred, target_names=['Negative', 'Neutral', 'Positive'], output_dict=True)
# Added output_dict=True to get a dictionary instead of a formatted string

# Convert the report into a DataFrame for easy plotting
report_df = pd.DataFrame(report).transpose()

# Extract the precision, recall, and F1-score values (excluding the 'support' row)
metrics = report_df.loc[['Negative', 'Neutral', 'Positive'], ['precision', 'recall', 'f1-score']]

# Plot the metrics using a bar chart
metrics.plot(kind='bar', figsize=(10, 6))
plt.title('Precision, Recall, and F1-Score for Each Class in META LEARNER')
plt.xlabel('Classes')
plt.ylabel('Score')
plt.ylim(0, 1)  # Precision, recall, and F1-score values range between 0 and 1
plt.xticks(rotation=0)  # Keep x-axis labels horizontal for readability
plt.grid(True, alpha=0.3)
plt.legend(loc='upper right', title='Metrics')

# Show the plot
plt.tight_layout()
plt.show()

# Training and validation accuracy
train_accuracy = meta_learner.score(X_train, y_train)
test_accuracy = meta_learner.score(X_test, y_test)

# Print training and validation accuracy
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Validation Accuracy: {test_accuracy:.4f}")

# If you want to simulate a training curve, you need to track it manually using a loop and store the scores at each iteration.

import numpy as np
import matplotlib.pyplot as plt

# Simulated data
epochs = range(1, 21)  # Simulating 20 "epochs"
training_losses = np.linspace(0.4, 0.2, len(epochs))  # Simulated training losses over 20 "epochs"
validation_losses = np.linspace(0.5, 0.25, len(epochs))  # Simulated validation losses over 20 "epochs"
train_accuracy = 0.85  # Example training accuracy
test_accuracy = 0.82  # Example test accuracy
training_accuracies = np.linspace(train_accuracy - 0.05, train_accuracy, len(epochs))
validation_accuracies = np.linspace(test_accuracy - 0.05, test_accuracy, len(epochs))

# Create a figure with two subplots side by side
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Plot Training and Validation Loss Curve
ax1.plot(epochs, training_losses, label='Training Loss')
ax1.plot(epochs, validation_losses, label='Validation Loss')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Loss')
ax1.set_title('Training and Validation Loss for Meta Learner')
ax1.legend()
ax1.grid(True, alpha=0.3)  # Adjust the alpha value to make the grid lines lighter

# Plot Training and Validation Accuracy Curve
ax2.plot(epochs, training_accuracies, label='Training Accuracy')
ax2.plot(epochs, validation_accuracies, label='Validation Accuracy')
ax2.set_xlabel('Epochs')
ax2.set_ylabel('Accuracy')
ax2.set_title('Training and Validation Accuracy for Meta Learner')
ax2.legend()
ax2.grid(True, alpha=0.3)  # Adjust the alpha value to make the grid lines lighter

# Show the combined plots
plt.tight_layout()
plt.show()

# Predict on the test set to get the confusion matrix
y_test_pred = meta_learner.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_test_pred)

# Display the confusion matrix using Matplotlib and Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral','Positive'], yticklabels=['Negative','Neutral', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Meta-Learner')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.preprocessing import label_binarize

# Simulated data
# Replace these with your model's true labels and predicted probabilities.
y_true = np.random.randint(0, 3, size=100)  # Example true labels for 3 classes: 0 (negative), 1 (neutral), 2 (positive)
y_scores = np.random.rand(100, 3)  # Example predicted probabilities for each class

# Binarize the true labels for multi-class Precision-Recall calculation
n_classes = y_scores.shape[1]
y_true_binarized = label_binarize(y_true, classes=np.arange(n_classes))

# Define class names for labeling
class_names = ['Negative', 'Neutral', 'Positive']

# Plot Precision-Recall curve for each class
plt.figure(figsize=(14, 8))
for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_true_binarized[:, i], y_scores[:, i])
    average_precision = average_precision_score(y_true_binarized[:, i], y_scores[:, i])
    plt.plot(recall, precision, label=f'{class_names[i]} (AP = {average_precision:.4f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Each Class')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Get the prediction probabilities for each class
y_pred_prob = meta_learner.predict_proba(X_test)

# Assuming binary classification with probabilities for each class
plt.figure(figsize=(10, 6))
sns.histplot(y_pred_prob[:, 1], bins=20, kde=True, color='green', label='Positive')
sns.histplot(y_pred_prob[:, 0], bins=20, kde=True, color='red', label='Negative')

sns.histplot(y_pred_prob[:, 2], bins=20, kde=True, color='blue', label='Neutral')
plt.xlabel('Prediction Probability')
plt.ylabel('Frequency')
plt.title('Model Prediction Distribution in Meta learner')
plt.legend()
plt.show()

# For multi-class, you could create separate histograms for each class.

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from itertools import cycle

# Assuming you have your true labels (y_test) and predicted probabilities (y_pred_prob)
# y_test is the true labels, y_pred_prob is the predicted probabilities for each class.

# Binarize the output
y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
n_classes = y_test_binarized.shape[1]
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Define class names for better labeling
class_names = ['Negative', 'Neutral', 'Positive']

# Plot ROC curve for each class
plt.figure(figsize=(10, 6))
colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of {0} (area = {1:0.2f})'.format(class_names[i], roc_auc[i]))

# Plot the diagonal line for random guessing
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Meta-Learner')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

# prompt: write code for Feature Maps

# Assuming you have your meta-learner and the feature importance attributes
feature_importance = meta_learner.coef_[0]  # Assuming a logistic regression model

# Create a DataFrame to store the feature names and their importance scores
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# Sort the DataFrame by importance scores in descending order
feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)

# Plot the feature importance
plt.figure(figsize=(10, 6))
plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Features')
plt.ylabel('Importance Score')
plt.title('Feature Importance in Meta-Learner')
plt.xticks(rotation=45, ha='right')
plt.grid(True, alpha=0.3)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Assuming you have your true labels (y_test) and predicted labels (y_test_pred)

# Create a DataFrame to store the true and predicted labels
results_df = pd.DataFrame({'True_Label': y_test, 'Predicted_Label': y_test_pred})

# Filter out the incorrect predictions
misclassified_df = results_df[results_df['True_Label'] != results_df['Predicted_Label']]

# Count the occurrences of each type of misclassification
misclassification_counts = misclassified_df.groupby(['True_Label', 'Predicted_Label']).size().reset_index(name='Count')

# Plot the misclassification counts as a bar chart
plt.figure(figsize=(10, 6))
plt.bar(misclassification_counts.index, misclassification_counts['Count'])
plt.xlabel('Misclassification Type (True Label, Predicted Label)')
plt.ylabel('Count')
plt.title('Error Analysis: Misclassification Counts')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.grid(True, alpha=0.3)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Get the predicted probabilities
y_pred_probs = meta_learner.predict_proba(X_test)

# Get the predicted labels
y_pred = meta_learner.predict(X_test)

# Get the confidence scores (maximum probability for each prediction)
confidence_scores = np.max(y_pred_probs, axis=1)

# Create a DataFrame for analysis
confidence_df = pd.DataFrame({'Confidence': confidence_scores, 'Prediction': y_pred, 'True_Label': y_test})

# Plot the distribution of confidence scores
plt.figure(figsize=(10, 6))
sns.histplot(confidence_df['Confidence'], bins=20, kde=True)
plt.xlabel('Confidence Score')
plt.ylabel('Frequency')
plt.title('Distribution of Model Confidence Scores')
plt.grid(True, alpha=0.3)
plt.show()

# Analyze confidence scores for correct and incorrect predictions
correct_predictions = confidence_df[confidence_df['Prediction'] == confidence_df['True_Label']]
incorrect_predictions = confidence_df[confidence_df['Prediction'] != confidence_df['True_Label']]

# Plot confidence distributions for correct and incorrect predictions
plt.figure(figsize=(10, 6))
sns.histplot(correct_predictions['Confidence'], bins=20, kde=True, color='green', label='Correct Predictions')
sns.histplot(incorrect_predictions['Confidence'], bins=20, kde=True, color='red', label='Incorrect Predictions')
plt.xlabel('Confidence Score')
plt.ylabel('Frequency')
plt.title('Confidence Scores for Correct and Incorrect Predictions')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Further analysis can be done by calculating metrics like accuracy, precision, and recall for different confidence intervals.